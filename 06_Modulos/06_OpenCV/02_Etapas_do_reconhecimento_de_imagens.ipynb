{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapas do Processo de Reconhecimento de Imagens\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos  quais  são  as  etapas  principais  do  processo  de  reconhecimento  de  imagens. Mas antes, um pouco de história sobre detecção de objetos e reconhecimento de imagens.\n",
    "\n",
    "Essa história começa em 2001, o ano em que um algoritmo eficiente para detecção de faces foi  inventado  por  Paul  Viola  e  Michael  Jones.  Sua  demonstração  que  mostrou  rostos sendo  detectados  em  tempo  real  em  um  feed  de  webcam, foi  a  demonstração  mais impressionante  de  visão computacional  e  seu  potencial  naquele momento.  Em seguida,  foi implementado  no  OpenCV  e  a  detecção  de  rosto  tornou-se  sinônimo  de  algoritmo  Viola  e Jones.\n",
    "\n",
    "De tempos em tempos, uma nova ideia surgee normalmente ajuda a transformar uma tecnologia e dar um salto nas pesquisas. Na detecção de objetos, essa ideia veio em 2005 com uma pesquisa de Navneet Dalal e Bill Triggs. Seu descritor de características, o Histograma de Gradientes Orientados (HOG), superou significativamente os algoritmos existentes na detecção de pedestres.\n",
    "\n",
    "A cada década surge uma nova ideia que é tão eficaz e poderosa que você abandona tudo o que veio antes e abraça a nova tecnologia. Deep Learning é a ideia desta década. Os algoritmos  de  Aprendizagem  Profunda existem há muito  tempo,  mas  eles  se  tornaram main stream  na  visão  computacional  com  seu  sucesso  ressonante  no  ImageNet  Large  Scale Visual Recognition Challenge (ILSVRC) de 2012.\n",
    "\n",
    "Nessa competição, um algoritmo baseado em Deep Learning desenvolvido por Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton sacudiram o mundo  da  visão computacional com uma  incrivel  precisão  de  85% - 11%  melhor  do  que  o algoritmo que ganhou o segundo lugar! No ILSVRC 2012, esta foi a única pesquisa baseada em Aprendizagem  Profunda.  Em  2013,  todas  as pesquisas vencedoras  foram  baseadas  em Aprendizagem Profunda e, em 2015, vários algoritmos baseados na Rede Neural Convolucional (CNN) ultrapassaram a taxa de reconhecimento humano de 95%\n",
    "\n",
    "Com  tão  grande  sucesso  no  reconhecimento  de  imagens,  a  detecção  de  objetos baseados  em  Aprendizagem  Profunda  era  inevitável.  As  técnicas  como  a  Faster R-CNN produzem resultados impressionante sem várias classes de objetos. Tenha em mente que, se você não está usando os algoritmos de reconhecimento de imagem e algoritmos de detecção de objetos baseados em Deep Learning, você pode estar perdendo uma grande oportunidade para obter melhores resultados.\n",
    "\n",
    "Mas  antes  de  mergulhar  em  Deep  Learning,  vamos entender  o  reconhecimento  de imagem usando técnicas tradicionais de visão computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Reconhecimento de Imagens (ou Classificação de Imagens)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um  algoritmo  de  reconhecimento  de  imagem  (também  chamado  de classificador  de imagem) recebe uma imagem como entrada e gera como saída o que a imagem contém. Em outras palavras, a saída é um rótulo de classe (por exemplo, \"gato\", \"cachorro\", \"fruta\" etc.). Como  um  algoritmo  de  reconhecimento  de  imagem  conhece  o  conteúdo  de  uma  imagem? Bem, você precisa treinar o algoritmo para aprender as diferenças entre as diferentes classes. Se  você  quer  encontrar  gatos  em  imagens,  você  precisa  treinar  um  algoritmo  de reconhecimento de imagem com milhares de imagens de gatos e milhares de imagens que não contenham gatos. Esse algoritmo só pode entender objetos/classes que ele aprendeu.\n",
    "\n",
    "Para  simplificar  as  coisas, vamos  nos  concentrar  em classificadores  de  duas  classes (binários). Você pode pensar que esta é uma suposição muito limitante, mas tenha em mente que muitos detectores de objetos populares (por exemplo, detector de rosto e detector de pedestres) têm um classificador binário sob o capô. Por exemplo: dentro de um detector de rosto há um classificador de imagem que diz se uma imagem é um rosto ou não.\n",
    "\n",
    "Este diagrama ilustra os passos em um processo tradicional de classificação de imagens:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56327670-c2cd7200-6151-11e9-8edd-c1b6d5c8674e.png)\n",
    "\n",
    "Curiosamente,  muitos  algoritmos  tradicionais  de  classificação  de  imagem  de  visão computacional seguem  este  pipeline,  enquanto  os  algoritmos  baseados  em  Deep  Learning ignoram  completamente  o  passo  de  extração  de  recursos.  Vejamos  estes  passos  em  mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Passo 1: Pré-Processamento\n",
    "***\n",
    "\n",
    "Muitas vezes, uma imagem de entrada é pré-processada para normalizar os efeitos de contraste  e  brilho.  Um  passo  de  pré-processamento  muito  comum  é  subtrair  a  média  das intensidades de imagem e dividir pelo desvio padrão. Às vezes, a correção de gama produz resultados  ligeiramente  melhores.  Ao  lidar  com  imagens  em  cores,  uma  transformação  do espaço  de  cores  (por  exemplo,  espaço  de  cores  RGB  para Grayscale)  pode  ajudar  a  obter melhores resultados.\n",
    "\n",
    "Observe que não prescrevemos quais etapas de pré-processamento são boas. A razão é que  ninguém  sabe  antecipadamente  quais  desses  passos  de  pré-processamento  produzirão bons  resultados.  Você  tenta  alguns  diferentes  e  alguns  podem  dar  resultados  um  pouco melhores. Aqui está um parágrafo de Dalal e Triggs\n",
    "\n",
    "\"Nós  avaliamos  várias  representações  de  pixels  de  entrada,  incluindo  espaços  de  cores  em escala de cinza, RGB e LAB opcionalmente com equalização de lei de potência (gama). Essas normalizações  têm  apenas  um  efeito  modesto  sobre  o  desempenho,  talvez  porque  a subsequente  normalização  do  descritor  tenha  resultados  semelhantes.  Nós  usamos informações de cores quando disponíveis. Os espaços de cores RGB e LAB oferecem resultados comparáveis, mas restringir a escala de cinza reduz o desempenho em 1,5% em 10-4 FPPW. A compressão de  raiz  quadrada  de  gama de  cada  canal  de  cores  melhora  o  desempenho  em FPPW baixo (em 1% a 10-4 FPPW), mas a compressão de log é muito forte e piora em 2% em 10-4 FPPW\"\n",
    "\n",
    "Como você pode ver, eles não sabiam antecipadamente qual pré-processamento usar. Eles fizeram suposições razoáveis e foram usando como tentativa e erro.\n",
    "\n",
    "Como parte do pré-processamento, uma imagem de entrada ou patch de uma imagem também é cortada e redimensionada para um tamanho fixo. Isso é essencial porque o próximo passo, extração de recursos, é executado em uma imagem de tamanho fixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Etapa 2: Extração de Recursos\n",
    "***\n",
    "\n",
    "A  imagem  de  entrada  possui  muita  informação  extra  que  não  é  necessária  para  a classificação. Portanto, o primeiro passo na classificação da imagem é simplificar a imagem, extraindo  a  informação  importante  contida  na  imagem  e  deixando  de  lado  o  resto.  Por exemplo, se você quiser encontrar botões de camisa e casaco nas imagens, você notará uma variação significativa nos valores de pixels RGB. No entanto, ao executar um detector de borda em uma imagem, podemos simplificar a imagem. Você ainda pode discernir facilmente a forma circular dos botões nessas imagens de borda e, portanto, podemos concluir que a detecção de bordas mantém as informações essenciais enquanto descarta informações não essenciais. O passo é chamado de extração de recursos. \n",
    "\n",
    "Nas abordagens tradicionais de visão computacional, conceber esses recursos são cruciais para o desempenho do algoritmo. Acontece que podemos fazer muito melhor do que simples detecção de bordas e encontrar recursos que são muito mais confiáveis. No nosso exemplo de botões de camisa e casaco, um bom detector de recursos não só capturará a forma circular dos botões, mas também informações sobre como os botões são  diferentes  de  outros  objetos  circulares,  como  pneus  de  carro. Alguns  recursos  bem conhecidos utilizados na visão computacional são:\n",
    "\n",
    "* Haar-like features,introduzidas por Viola e Jones\n",
    "* Histogram of Oriented Gradients(HOG)\n",
    "* Scale-Invariant Feature Transform(SIFT)\n",
    "* Speeded Up Robust Feature(SURF)\n",
    "\n",
    "Uma das formas mais comuns de extração de recursos é o Histograma de Gradientes Orientados (HOG).\n",
    "\n",
    "Um algoritmo de extração de recursos converte uma imagem de tamanho fixo em um vetor de características de tamanho fixo. No caso da detecção de pedestres, o descritor de características de HOG é calculado para um patch de 64 × 128 de uma imagem e retorna um vetor de tamanho 3780. Observe que a dimensão original deste patch de imagem seria de 64 x 128 x 3 = 24.576 que é reduzido para 3780 pelo descritor HOG.\n",
    "\n",
    "HOG  baseia-se  na  ideia  de  que  a  aparência  do  objeto  local  pode  ser  efetivamente descrita  pela  distribuição  (histograma)  das  direções  de  borda  (gradientes  orientados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Passo 3: Algoritmo de Aprendizagem para Classificação\n",
    "***\n",
    "\n",
    "Antes que um algoritmo de classificação possa fazer sua magia, precisamos treiná-lo mostrando milhares de exemplos de gatos e fundos (backgrounds). Diferentes algoritmos de aprendizagem  aprendem  de  forma  diferente,  mas  o  princípio  geral  é  que  os  algoritmos de aprendizagem  tratam  os  vetores  de  características  como  pontos  no  espaço  dimensional superior e tentam encontrar planos / superfícies que dividem o espaço dimensional superior de tal  maneira  que  todos  os  exemplos  pertencentes  à  mesma  classe estejam  em  um  lado  do plano/superfície.\n",
    "\n",
    "Para  simplificar  as  coisas,  vejamos  um  algoritmo  de  aprendizado  chamado  Support Vector Machines (SVM) com algum detalhe. Como o Support Vector Machine (SVM) funciona para a classificação da imagem?\n",
    "\n",
    "O Support Vector Machine (SVM) é um dos algoritmos de aprendizagem supervisionada para classificação binária mais populares. Embora as ideias usadas na SVM tenham existido desde 1963, a versão atual foi proposta em 1995 por Cortes e Vapnik.\n",
    "\n",
    "No passo anterior, aprendemos que o descritor HOG de uma imagem é um vetor de características do comprimento 3780. Podemos pensar neste vetor como um ponto em um espaço de 3780-dimensional. Visualizando espaço dimensional maior é impossível, então vamos simplificar  as coisas  um  pouco  e  imaginar  que  o  vetor  de  características  era  apenas bidimensional.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56327944-0ecce680-6153-11e9-8334-438f7fbc2081.png)\n",
    "\n",
    "Em nosso mundo simplificado, agora temos pontos 2D representando as duas classes (por exemplo, gatos e fundo). Na imagem acima, as duas classes são representadas por dois tipos  diferentes  de  pontos.  Todos  os  pontos pretos pertencem  a  uma  classe  e  os  pontos brancos  pertencem  à  outra  classe.  Durante  o  treinamento, alimentamos o  algoritmo  com muitos exemplos das duas classes. Em outras palavras, damos ao algoritmo as coordenadas dos pontos 2D e também se o ponto é preto ou branco.\n",
    "\n",
    "Diferentes algoritmos de aprendizagem descobrem como separar essas duas classes de maneiras diferentes. O SVM linear tenta encontrar a melhor linha que separa as duas classes. Na figura acima, H1, H2 e H3 são três linhas neste espaço 2D. H1 não separa as duas classes e, portanto, não é um bom classificador. H2 e H3 separam as duas classes, mas, intuitivamente, parece que H3 é um classificador melhor que o H2 porque o H3 parece separar as duas classes de forma mais limpa. Por quê? H2 está muito perto de alguns dos pontos preto e branco. Por outro lado, H3 é escolhido de modo que esteja a uma distância máxima dos membros das duas classes.\n",
    "\n",
    "Dado os recursos 2D da figura acima, o SVM encontrará a linha H3 para você. Se você receber um novo vetor de recursos 2D correspondente a uma imagem que o algoritmo nunca viu antes, você pode simplesmente testar qual lado da linha está o ponto e atribuir-lhe o rótulo da classe apropriado. Se seus vetores de características estiverem em 3D, o SVM encontrará o plano apropriado que separa as duas classes. Como você pode ter adivinhado, se seu vetor de recurso  estiver  em  um  espaço  de  3780-dimensional,  o  SVM  encontrará  o  hiperplano apropriado.\n",
    "\n",
    "De  forma  geral,  essas são  as  principais  etapas  no  processo  de  reconhecimento  de imagens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
