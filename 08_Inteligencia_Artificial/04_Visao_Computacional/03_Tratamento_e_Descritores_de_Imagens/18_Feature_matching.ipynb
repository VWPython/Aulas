{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching\n",
    "***\n",
    "\n",
    "Vamos mostrar qual o objetivo de aprender esses algorítmos explicadas nos itens anteriores para extrair os recursos de pontos de interesse dentro de regiões especificas de uma imagem e a partir dai criar nosso vetor de recursos em tarefas de pré-processamento em visão computacional.\n",
    "\n",
    "Para o computador verificar se 2 imagens são iguais ou não, primeiro ele tem que extrair o vetor de recursos das duas imagens, detectar os pontos de interesse (o que é mais relevante dentro da imagem) e compara-los. Com isso ele vai dizer se ambas as imagens são iguais ou não.\n",
    "\n",
    "O objetivo do Feature matching é comparar os recursos de uma imagem com os recursos de outra imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def show(img, title=\"\"):\n",
    "    \"\"\"\n",
    "    Redimensionar a imagem e mostrar no codigo.\n",
    "    \"\"\"\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.rcParams['figure.figsize'] = (50, 50)\n",
    "    plt.title(title, fontdict={\"fontsize\": 100})\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_matches(img1, keypoints1, img2, keypoints2, matches):\n",
    "    \"\"\"\n",
    "    Desenha os pontos de interesse que são iguais nas duas imagens.\n",
    "    retorna uma imagem de saida com a junção das 2 imagens e seus pontos de interesse.\n",
    "    \"\"\"\n",
    "\n",
    "    rows1, cols1 = img1.shape[:2]\n",
    "    rows2, cols2 = img2.shape[:2]\n",
    "\n",
    "    # Cria uma nova imagem de saída que concatene as duas imagens em conjunto\n",
    "    output_img = np.zeros((max([rows1,rows2]), cols1+cols2, 3), dtype='uint8')\n",
    "    output_img[:rows1, :cols1, :] = np.dstack([img1, img1, img1])\n",
    "    output_img[:rows2, cols1:cols1+cols2, :] = np.dstack([img2, img2, img2])\n",
    "\n",
    "    # Desenha linhas de conexão entre pontos-chave correspondentes\n",
    "    for match in matches:\n",
    "        # Obtém os pontos-chave correspondentes para cada uma das imagens\n",
    "        img1_idx = match.queryIdx\n",
    "        img2_idx = match.trainIdx\n",
    "\n",
    "        (x1, y1) = keypoints1[img1_idx].pt\n",
    "        (x2, y2) = keypoints2[img2_idx].pt\n",
    "\n",
    "        # Desenha um pequeno círculo em ambas as coordenadas e, em seguida, desenhe uma linha\n",
    "        radius = 4\n",
    "        colour = (0,255,0)   # green \n",
    "        thickness = 1\n",
    "        cv2.circle(output_img, (int(x1),int(y1)), radius, colour, thickness)   \n",
    "        cv2.circle(output_img, (int(x2)+cols1,int(y2)), radius, colour, thickness)\n",
    "        cv2.line(output_img, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), colour, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('imagens/bus-rotated.png', 0)  # query Image\n",
    "show(img1, \"Onibus rotacionado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread('imagens/bus.png', 0) # train Image\n",
    "show(img2, \"Onibus original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB Detector\n",
    "orb = cv2.ORB_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai keypoints e descritores\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(img2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto para fazer o match (objeto para fazer a comparação entre os vetores de recursos)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match descriptors (compara os descritores de ambas e gera suas combinações)\n",
    "matches = bf.match(descriptors1, descriptors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifica as combinações geradas na ordem de sua distância\n",
    "matches = sorted(matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desenha as primeiras 'n' matches\n",
    "img3 = draw_matches(img1, keypoints1, img2, keypoints2, matches[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img3, 'keypoints combinados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse algoritmo vai olhar para os pontos de interesse que nós customizamos, esses pontos de interesses geram os descritores de recursos que então vão gerar o vetor de recursos (valores números que representam cada uma dessas áreas de interesse).\n",
    "![img](https://user-images.githubusercontent.com/14116020/72210105-0eb90780-3495-11ea-8f29-78c56024994c.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
