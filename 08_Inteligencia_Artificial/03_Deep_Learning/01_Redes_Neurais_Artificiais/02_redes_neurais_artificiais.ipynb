{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoria sobre as redes neurais artificiais\n",
    "\n",
    "Aqui nós veremos vários cálculos de como que efetivamente uma rede neural faz para classificar um novo registro e como ela faz para encontrar o melhor conjunto de pesos.\n",
    "\n",
    "***\n",
    "### Perceptron de uma camada\n",
    "***\n",
    "\n",
    "O primeiro conceito importante é sobre **neurônio artificial** que é representado pela imagem abaixo:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56449030-03142800-62eb-11e9-8d0e-c9e3e9df416b.png)\n",
    "\n",
    "Na qual nós temos as entradas, e cada uma delas terá um peso correspondente, com isso utilizamos uma função de soma e ativação (f). O primeiro passo de uma rede neural é fazer-mos a aplicação de uma função soma:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56449065-6bfba000-62eb-11e9-8d25-faf496eee85e.png)\n",
    "\n",
    "que é a somatória de 1 a N de cada entrada (xi) multiplicada pelos respectivos pesos (wi), ou seja, na imagem acima temos: $soma = (1 * 0.8) + (7 * 0.1) + (5 * 0) = 0.8 + 0.7 + 0 = 1.5$. Logo após iremos aplicar a função de ativação que no caso é a **step funcion**, que é a função de ativação mais simples, também chamada de função degrau, que quando a função de soma for maior que zero ele retorna 1 (Verdadeiro) e caso contrário retorna 0 (Falso). Como o valor da soma é 1.5 o valor da saída do neurónio é igual a 1 (Verdadeiro).\n",
    "\n",
    "Como exemplo, vamos fazer a previsão de saída do operador AND:\n",
    "\n",
    "```\n",
    "X & Y = Z\n",
    "---------\n",
    "0 & 0 = 0\n",
    "0 & 1 = 0\n",
    "1 & 0 = 0\n",
    "1 & 1 = 1\n",
    "```\n",
    "\n",
    "Para pesos igual a **0**\n",
    "\n",
    "* $x = 0$ & $y = 0$ -> $0\\times0 + 0\\times0 = 0$\n",
    "* $x = 0$ & $y = 1$ -> $0\\times0 + 1\\times0 = 0$\n",
    "* $x = 1$ & $y = 0$ -> $1\\times0 + 0\\times0 = 0$\n",
    "* $x = 0$ & $y = 1$ -> $1\\times0 + 1\\times0 = 0$\n",
    "\n",
    "Com isso verificamos os resultados com a formula $Z - C = E$, na qual Z é a resposta esperada, C é a resposta obtida e E é se há erro (1) ou não (0)\n",
    "\n",
    "```\n",
    "Z - C = E\n",
    "0 - 0 = 0 (correto)\n",
    "0 - 0 = 0 (correto)\n",
    "0 - 0 = 0 (correto)\n",
    "1 - 0 = 1 (erro)\n",
    "```\n",
    "\n",
    "Com isso detectamos que o neurónio tem 75% de acerto e 25% de erro, porém queremos 100% de acerto, logo vamos calcular novos pesos de acordo com a parte que deu erro, com taxa_de_aprendizagem definida com 0.1:\n",
    "\n",
    "$$peso(n+1) = peso(n) + (taxaDeAprendizagem \\times entrada \\times erro) = 0 + (0.1 \\times 1 \\times 1) = 0.1$$\n",
    "\n",
    "Com isso iremos continuar o processo até achar o peso que tem como resultado o valor esperado, que no caso é 0.5\n",
    "\n",
    "* $x = 0$ & $y = 0$ -> $0\\times0.5 + 0\\times0.5 = 0$\n",
    "* $x = 0$ & $y = 1$ -> $0\\times0.5 + 1\\times0.5 = 0.5$\n",
    "* $x = 1$ & $y = 0$ -> $1\\times0.5 + 0\\times0.5 = 0.5$\n",
    "* $x = 0$ & $y = 1$ -> $1\\times0.5 + 1\\times0.5 = 1$\n",
    "\n",
    "Agora o resultado ta 100%\n",
    "\n",
    "```\n",
    "Z - C = E\n",
    "0 - 0 = 0 (correto)\n",
    "0 - 0 = 0 (correto)\n",
    "0 - 0 = 0 (correto)\n",
    "1 - 1 = 0 (correto)\n",
    "```\n",
    "\n",
    "A base da rede neural é descobrir quais são os pesos que devem ser utilizados para gerar o melhor resultado da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Redes multicamadas\n",
    "***\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56449538-06aaad80-62f1-11e9-80ce-11a0a47d195e.png)\n",
    "\n",
    "Os 3 exemplos acima são os casos de AND, OR e XOR respectivamente e o AND/OR são **linearmente separados**, ou seja, podemos inserir uma linha para separar os resultados 0 dos resultados 1, já o XOR não é linearmente separados, já que não podemos traçar essa linha. Só utilizaremos o perceptron para uma camada em problemas que pode ser separados por uma linha os resultados, caso não dê temos que usar **redes de multicamadas** ou **multilayer perceptron**.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56449559-76209d00-62f1-11e9-8b5d-a17f6eff2636.png)\n",
    "\n",
    "Nas redes de multicamadas temos várias camadas ocultas para gerar uma camada final. Para isso iremos utilizar outra função de ativação chamada **função sigmoide**.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56449571-b41dc100-62f1-11e9-95ef-26f4728a6a03.png)\n",
    "\n",
    "A função sigmoide é uma das funções mais utilizadas em redes neurais, em que o valor de x é nossa entrada, e ela retorna valores entre 0 e 1, como especificado no gráfico, se X for alto o valor será aproximadamente 1 e se X for pequeno o valor será aproximadamente 0 e não retorna valores negativos.\n",
    "\n",
    "Agora vamos fazer o exemplo, porém para o operador XOR:\n",
    "\n",
    "```\n",
    "X ^ Y = Z\n",
    "---------\n",
    "0 ^ 0 = 0\n",
    "0 ^ 1 = 1\n",
    "1 ^ 0 = 1\n",
    "1 ^ 1 = 0\n",
    "```\n",
    "\n",
    "Quando você vai trabalhar com uma rede neural os pesos são inicializados aleatóriamente e com os resultados vamos aprimorandos os pesos para gerar melhores resultados.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56449732-1aa3de80-62f4-11e9-8780-58362378fe9f.png)\n",
    "\n",
    "A gente faz esse processo para todas as possíveis entradas de dados até gerar o resultado do último nó da rede, com isso comparamos ao resultado que queremos e vimos que no caso do ```0 ^ 1 = 1``` não chegou perto já que o resultado foi **0.437** o que ta bem longe de **1**.\n",
    "\n",
    "Agora vamos **calcular o erro**:\n",
    "\n",
    "```\n",
    "Z - C = E\n",
    "---------\n",
    "0 - 0.406 = -0.406\n",
    "1 - 0.432 = 0.568\n",
    "1 - 0.437 = 0.563\n",
    "0 - 0.458 = -0.458\n",
    "```\n",
    "\n",
    "Para eu saber o total de erro somamos tudo e dividimos pelo número de caso. Com isso temos a **média absoluta** dos erros que é **0.49**, ou seja, nossa rede neural com esses pesos aleatórios no momento gera 49% de erro e 51% de acerto, temos que melhorar isso, ou seja, temos que encontrar o melhor conjunto de pesos que melhore a estimativa da rede neural para aproximadamente 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Descida do gradiente\n",
    "***\n",
    "\n",
    "Esse algoritmo de **descida do gradiente** é usado para gerar novos pesos de acordo com a média de erro geradas anteriormentes para gerar melhores resultados.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56449954-f990bd00-62f6-11e9-80dc-bba3e2cbab62.png)\n",
    "\n",
    "O ponto preto da imagem é o erro absoluto gerado no exemplo anterior, com isso precisamos ajustar os pesos para que ele vai descendo e encoste na base do gradiente (parte de baixo) onde o erro é pequeno, por isso ele é chamado de algoritmo de descida do gradiente.\n",
    "\n",
    "Um exemplo é jogar uma bolinha (erro) em uma tigela, a bolinha irá ficar oscilando na tigela até parar na base dela, é a mesma ideia, por meio de um loop iremos modificar os pesos até o erro chegar na base do gradiente.\n",
    "\n",
    "O **minC(w1, w2, ..., wn)** é o nosso **Loss function** ou **Minimo Custo** é uma função que **calcula a derivada parcial** para mover o erro para a direção do gradiente, ou seja, ela vai atualizar os pesos até que o erro seja mínimo.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56450096-a28be780-62f8-11e9-81df-47d5d8025c64.png)\n",
    "\n",
    "O calculo do gradiente é usado para encontrar a combinação de pesos que o erro é o menor possível, o gradiente é calculado para saber o quanto você deve ajustar os pesos para que ele se movimente para o mínimo global e não para o mínimo local como descrito no gráfico acima. Esse calculo é realizado pelo declive da curva com derivadas parciais.\n",
    "\n",
    "$$d = y * (1 - y)$$\n",
    "\n",
    "Em que  o y é o resultado da função de ativação. É esse valor da derivada (d) que irá dizer a direção que você vai percorrer no gradiente, ou seja, se você vai aumentar ou diminuir o valor do peso para que ele consiga se adaptar melhor aos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Cálculo da derivada (parâmetro delta)\n",
    "***\n",
    "\n",
    "Aqui iremos verificar como será os calculos para atualização dos pesos para decida no gradiente. Para isso temos o seguinte roteiro: **Função de ativação**, **Derivada da função**, **Delta**, **Gradiente**. O gradiente que vai indicar qual direção vamos seguir, se vamos aumentar ou diminuir os pesos.\n",
    "\n",
    "Para o caso ```0 ^ 1 = 1``` temos:\n",
    "\n",
    "### Função de ativação\n",
    "\n",
    "$$y = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "```\n",
    "Soma = -0.274\n",
    "Ativação = 0.432\n",
    "Error = 0.568\n",
    "```\n",
    "\n",
    "### Derivada da função\n",
    "\n",
    "$$derivadaSigmoide = y * (1 - y)$$\n",
    "\n",
    "```\n",
    "derivadaSigmoide = 0.432 * (1 - 0.432) = 0.245\n",
    "```\n",
    "\n",
    "### Delta\n",
    "\n",
    "$$deltaDeSaida = erro * derivadaSigmoide$$\n",
    "\n",
    "```\n",
    "deltaDeSaida = 0.568 * 0.245 = 0.139\n",
    "```\n",
    "\n",
    "Pegamos o delta para todos os neurônios artificiais de todas as camadas ocultas, para ajustar os pesos de todos eles.\n",
    "\n",
    "$$deltaDaCamadaOculta = derivadaSigmoide * peso * deltaDeSaida$$\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56450649-e7664d00-62fd-11e9-8bf7-143a0d49ccac.png)\n",
    "\n",
    "### Gradiente\n",
    "\n",
    "Vamos atualizar os pesos com o **backpropagation**, pela formula temos, o próximo peso é igual ao peso atual vezes o momento (parâmetro adicional para acelerar o processo de descida do gradiente, porém tem otimizador que não usa esse parâmetro, como o otimizador \"adam\") mais a entrada * o delta de saída vezes a taxa de aprendizagem (normalmente definida como 0.001 na maioria das bibliotecas)\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56450791-b5091f80-62fe-11e9-9f06-385ccb3c62db.png)\n",
    "\n",
    "O algoritmo é chamado backpropagation porque ele vai utilizando o delta de saída e atualizando os pesos até chegar na camada de entrada. Ou seja, primeiro ele faz o processo até encontrar o erro e depois volta atualizando os pesos, o que é conhecido como **rede neural fit-forward**.\n",
    "\n",
    "Por exemplo:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451037-25fd0700-6300-11e9-8d2b-32ddc6c13948.png)\n",
    "\n",
    "Vamos fazer a somatória dos valores de (entrada * delta) de cada caso para cada camada oculta da rede neural:\n",
    "\n",
    "$$0.5 * (-0.098) + 0.589 * 0.139 + 0.396 * 0.139 + 0.484 * (-0.114) = 0.032$$\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451185-ea167180-6300-11e9-885f-da104b2d9f5a.png)\n",
    "\n",
    "$$0.5 * (-0.098) + 0.360 * 0.139 + 0.323 * 0.139 + 0.211 * (-0.114) = 0.022$$\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451259-3e215600-6301-11e9-9ecd-aa1c449850e1.png)\n",
    "\n",
    "$$0.5 * (-0.098) + 0.385 * 0.139 + 0.277 * 0.139 + 0.193 * (-0.114) = 0.021$$\n",
    "\n",
    "Com isso vamos calcular os próximos pesos com os seguintes dados:\n",
    "\n",
    "```\n",
    "taxa_de_aprendizagem = 0.3\n",
    "momento = 1\n",
    "entrada * delta = 0.032, 0.022, 0.021\n",
    "```\n",
    "\n",
    "O momento com valor 1 não altera nada. Vamos verificar somento os novos pesos dos neuronios abaixo:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451438-06ff7480-6302-11e9-86b8-3927e0a7e347.png)\n",
    "\n",
    "```\n",
    "(-0.017 * 1) + 0.032 * 0.3 = -0.007\n",
    "(-0.893 * 1) + 0.022 * 0.3 = -0.886\n",
    "(0.148 * 1) + 0.021 * 0.3 = 0.154\n",
    "```\n",
    "\n",
    "Com isso temos o novo conjunto de pesos.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451605-e1bf3600-6302-11e9-8e71-74eb051410da.png)\n",
    "\n",
    "Com isso faremos o mesmo processo para as camadas de entrada:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451685-4bd7db00-6303-11e9-9ad4-dbede2d23720.png)\n",
    "\n",
    "Para cada uma das entrada iremos calcular os valores de atualização dos peso, a imagem já tem gerado os deltas para cada um deles.\n",
    "\n",
    "Com isso vamos atualizar os pesos:\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451732-92c5d080-6303-11e9-92f1-6db7083fbd31.png)\n",
    "\n",
    "Aplicando a mesma formula com as entradas temos:\n",
    "\n",
    "```\n",
    "taxa_de_aprendizagem = 0.3\n",
    "momento = 1\n",
    "entrada * delta = -0.000, -0.010, 0.001, -0.000, -0.012, 0.002\n",
    "```\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56451833-1384cc80-6304-11e9-9ca9-e1c27beab8ff.png)\n",
    "\n",
    "Com isso vamos realizar esse processo N vezes melhorando sempre os pesos e os resultados, e a cada loop desses nós chamamos de **épocas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Outros conceitos importantes\n",
    "***\n",
    "\n",
    "### Bias\n",
    "\n",
    "Unidade de **bias** é um neuronio adicionar que colocamos na rede, a maioria das bibliotecas de redes neurais já vem com essa unidade implementada que serve para quando se tem todas as entradas igual a zero, ele faz com que o valor da função soma não seja zero, ou seja, muda a saída com essa unidade.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56452563-2b5e4f80-6308-11e9-8eaa-0442786909b2.png)\n",
    "\n",
    "### Erro\n",
    "\n",
    "Algoritmo simples, não muito usado nas bibliotecas de redes neurais:\n",
    "\n",
    "$$erro = respostaCorreta - respostaCalculada$$\n",
    "\n",
    "Duas das formas mais robustas de calcular erros são: **Mean Square Error (MSE) e Root Mean Square Error (RMSE)**\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/14116020/56452638-b3912480-6309-11e9-9c99-88cc4129d03b.png)\n",
    "\n",
    "Soma = 1.011\n",
    "\n",
    "MSE = 1.011/4 = 0.252\n",
    "\n",
    "RMSE = 0.501\n",
    "\n",
    "### Descida do gradiente\n",
    "\n",
    "A duas formas de fazer isso, pegando a base toda calculando o erro ara todos os registros e atualiza os pesos (**Batch Gradient Descent (BGD)**) ou pegando registro por registro, calculando seu erro e mudando seu peso (**Stochastic Gradient Descent (SGD)**) esse por sua vez previne mínimos locais e é mais rápido, ou seja, não precisa carregar todos os dados em memória.\n",
    "\n",
    "Temos também o **Mini Batch Gradient Descent** que ao inves de pegar todos os registro ele pega blocos de registros, calcula o erro e atualiza os pesos. Ou seja, ele seleciona quantos registro que pegar por vez para fazer a atualização.\n",
    "\n",
    "### Parâmetros\n",
    "\n",
    "* **Learning Rate**: Taxa de aprendizagem\n",
    "\n",
    "* **Batch Size**: Tamanho do lote, por exemplo, quantidade de registros que será atualizado os pesos (mini batch gradient descent)\n",
    "\n",
    "* **Epochs**: Épocas, ou seja, quantidades de vezes que será atualizado/melhorado os pesos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
